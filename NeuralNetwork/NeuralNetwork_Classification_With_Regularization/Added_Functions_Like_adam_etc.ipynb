{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment 2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XoNilKhg-58H"
      },
      "source": [
        "# <center>CS568:Deep Learning</center>  <center>Spring 2020</center> "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df94ExRZtCO5",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Total Marks: 100** \n",
        "\n",
        "**Assigned **: Friday, April 17, 2020 \n",
        "\n",
        "**Due **: Friday, April 24, 2020 before **8.00 AM**\n",
        "\n",
        "## a) Initialization \n",
        "\n",
        "Train a two layer neural network for binary classification and fill the table given below. **(20 marks)**\n",
        "+ Dataset = MNIST\n",
        "+ Classes = 1 and 2 digits from MNIST\n",
        "+ No. of layers = 2 \n",
        "+ Activation function on each hidden layer = sigmoid \n",
        "+ Neurons in each layer = 5\n",
        "+ Batch size = 32 \n",
        "+ Epochs = 100 \n",
        "+ Learning rate = 1e-3 \n",
        "+ Optimizer = Stochastic gradient descent\n",
        "       \n",
        "<table> \n",
        "    <tr>\n",
        "        <td>\n",
        "        ** Initialization **\n",
        "        </td>\n",
        "        <td>\n",
        "        **Train accuracy**\n",
        "        </td> \n",
        "          <td>\n",
        "        **Test accuracy**\n",
        "        </td>         \n",
        "    </tr>\n",
        "        <td>\n",
        "       Zeros initialization\n",
        "        </td>\n",
        "        <td>\n",
        "         99.44%\n",
        "        </td> \n",
        "        <td>\n",
        "         99.35%\n",
        "        </td>         \n",
        "    <tr>\n",
        "        <td>\n",
        "       Random initialization  and c=0.01\n",
        "        </td>\n",
        "        <td>\n",
        "        99.71%\n",
        "        </td> \n",
        "        <td>\n",
        "         99.44%\n",
        "        </td> \n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "       Random initialization  and c=100\n",
        "        </td>\n",
        "        <td>\n",
        "        53.33%  No Training\n",
        "        </td>  \n",
        "         <td>\n",
        "         52.71% No Training\n",
        "        </td> \n",
        "    </tr>\n",
        "     <tr>\n",
        "        <td>\n",
        "        Xavier initialization \n",
        "        </td>\n",
        "        <td>\n",
        "        99.78%\n",
        "        </td>       \n",
        "         <td>\n",
        "         99.44%\n",
        "        </td> \n",
        "    </tr>\n",
        "</table>        \n",
        "       \n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY2-RydwtCO7",
        "colab_type": "text"
      },
      "source": [
        "## b) Optimizer\n",
        "\n",
        "Train a two layer neural network for binary classification and fill the table given below. **(50 marks)**\n",
        "+ Dataset = MNIST\n",
        "+ Classes = 1 and 2 digits from MNIST\n",
        "+ Initialization = Xavier\n",
        "+ No. of layers = 2 \n",
        "+ Activation function on each hidden layer = sigmoid \n",
        "+ Neurons in each layer = 5\n",
        "+ Batch size = 32 \n",
        "+ Epochs = 100 \n",
        "+ Learning rate = 1e-3 \n",
        "       \n",
        "<table> \n",
        "    <tr>\n",
        "        <td>\n",
        "        **Optimization method**\n",
        "        </td>\n",
        "        <td>\n",
        "        **Train accuracy**\n",
        "        </td> \n",
        "          <td>\n",
        "        **Test accuracy**\n",
        "        </td>         \n",
        "    </tr>\n",
        "        <td>\n",
        "        Gradient Descent\n",
        "        </td>\n",
        "        <td>\n",
        "         99.79%\n",
        "        </td> \n",
        "        <td>\n",
        "         99.44%\n",
        "        </td>         \n",
        "    <tr>\n",
        "        <td>\n",
        "        Stochastic Gradient Descent\n",
        "        </td>\n",
        "        <td>\n",
        "        99.77%\n",
        "        </td> \n",
        "        <td>\n",
        "         99.49%\n",
        "        </td> \n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "       Momentum\n",
        "        </td>\n",
        "        <td>\n",
        "        99.98%\n",
        "        </td>  \n",
        "         <td>\n",
        "         99.63%\n",
        "        </td> \n",
        "    </tr>\n",
        "     <tr>\n",
        "        <td>\n",
        "        Adam\n",
        "        </td>\n",
        "        <td>\n",
        "        99.96%\n",
        "        </td>       \n",
        "         <td>\n",
        "         99.67%\n",
        "        </td> \n",
        "    </tr>\n",
        "</table>        \n",
        "       "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "494-2RlhtCO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## add adam code here (30 marks)\n",
        "import numpy as np\n",
        "def update_parameters_adam(self,epoch):\n",
        "        beta1 = 0.9\n",
        "        beta2 = 0.999\n",
        "        eps_stable = 1e-8\n",
        "        for i in range(1,self.num_layers+1):\n",
        "            self.adammom[\"dW%s\"%i] = (beta1 * self.adammom[\"dW%s\"%i]) + ((1. - beta1) * self.grads[\"dW%s\"%i])\n",
        "            self.adammom['db%s'%i] = (beta1 * self.adammom['db%s'%i]) + ((1. - beta1) * self.grads[\"db%s\"%i])\n",
        "            self.adamv[\"dW%s\"%i] = (beta2 * self.adamv[\"dW%s\"%i]) + ((1. - beta2) * np.square(self.grads[\"dW%s\"%i]))\n",
        "            self.adamv['db%s'%i] = (beta2 * self.adamv['db%s'%i]) + ((1. - beta2) * np.square(self.grads[\"db%s\"%i]))\n",
        "            mom_prime_w = self.adammom[\"dW%s\"%i] / (1. - (beta1 ** i))\n",
        "            mom_prime_b = self.adammom['db%s'%i] / (1. - (beta1 ** i))\n",
        "            vm = self.adamv[\"dW%s\"%i] / (1. - (beta2 ** i)) \n",
        "            vb = self.adamv['db%s'%i] / (1. - (beta2 ** i)) \n",
        "            update_w = (self.learning_rate / (np.sqrt(vm) + eps_stable)) * mom_prime_w\n",
        "            update_b = (self.learning_rate / (np.sqrt(vb) + eps_stable)) * mom_prime_b\n",
        "            self.parameters['W%s' %i] = self.parameters['W%s' % i] - update_w\n",
        "            self.parameters['b%s' %i] = self.parameters['b%s' % i] - update_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTq2oqVdtCPK",
        "colab_type": "text"
      },
      "source": [
        "## c) Regularization\n",
        "\n",
        "Train a two layer neural network for binary classification and fill the table given below. **(30 marks)**\n",
        "+ Dataset = MNIST\n",
        "+ Classes = 1 and 2 digits from MNIST\n",
        "+ No. of layers = 2\n",
        "+ Initialization = Xavier\n",
        "+ Optimizer = SGD\n",
        "+ Activation function on each hidden layer = sigmoid \n",
        "+ Neurons in each layer = 5\n",
        "+ Batch size = 32 \n",
        "+ Epochs = 100 \n",
        "+ Learning rate = 1e-3 \n",
        "       \n",
        "<table> \n",
        "    <tr>\n",
        "        <td>\n",
        "        **Regularization method**\n",
        "        </td>\n",
        "        <td>\n",
        "        **Train accuracy**\n",
        "        </td> \n",
        "          <td>\n",
        "        **Test accuracy**\n",
        "        </td>         \n",
        "    </tr>\n",
        "        <td>\n",
        "        l2-weight with lembda = 0.5\n",
        "        </td>\n",
        "        <td>\n",
        "         98.63%\n",
        "        </td> \n",
        "        <td>\n",
        "         98.89%\n",
        "        </td>    \n",
        "        </tr>\n",
        "        <td>\n",
        "        l2-weight with lembda = 0.2\n",
        "        </td>\n",
        "        <td>\n",
        "         99.0%\n",
        "        </td> \n",
        "        <td>\n",
        "         99.4%\n",
        "        </td>         \n",
        "    <tr>\n",
        "        <td>\n",
        "        Dropout prob = [1,0.6,0.6,1]\n",
        "        </td>\n",
        "        <td>\n",
        "        99.22%,99.27\n",
        "        </td> \n",
        "        <td>\n",
        "         99.35%,99.44\n",
        "        </td> \n",
        "    </tr>  \n",
        "    <td>\n",
        "        Dropout prob = [1,0.8,0.8,1]\n",
        "        </td>\n",
        "        <td>\n",
        "        99.55%,99.51\n",
        "        </td> \n",
        "        <td>\n",
        "         99.40%,99.35\n",
        "        </td> \n",
        "    </tr>  \n",
        "</table>        "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNYjYWaatCPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add l2-weight and dropout code here (You can copy your fporp and bprop functions here) (20 marks)\n",
        "############################################l2-weight and DropOut Code##############################################\n",
        "#loss function for l2-weight\n",
        "def calculate_loss(self, batch_target,regu = True):\n",
        "        N = self.net1['A%s' %self.num_layers].shape[1]\n",
        "        ce = -np.sum(batch_target * np.log(self.net1['A%s' %self.num_layers])) / N\n",
        "        #l2 regulization\n",
        "        if(regu):\n",
        "            for i in range(1,self.num_layers+1):\n",
        "                ce += (self.lembda/2) * np.sum(np.square(self.parameters[\"W%s\" % i]))\n",
        "        return ce \n",
        "# fprob \n",
        "keep_prob = [1,0.6,0.6,1] # 3 out of 5 neurons will be active in both hidden layer\n",
        "def fprop(self, batch_input,traing = False,keep_prob = None):\n",
        "        self.net1['A0'] = batch_input\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            self.net['Z%s' %i] = self.parameters['W%s' %i].dot(self.net1['A%s'%(i-1)]) + self.parameters['b%s' %i]\n",
        "            self.net1['A%s' %i] = eval(self.activations_func[i])(self.net['Z%s'%i])\n",
        "            #Dropout\n",
        "            if(traing):\n",
        "                self.drop[str(i)] = np.random.rand(self.net1['A%s' %i].shape[0], self.net1['A%s' %i].shape[1])\n",
        "                self.drop[str(i)] = self.drop[str(i)] < keep_prob[i]\n",
        "                self.net1['A%s' %i] = np.multiply(self.net1['A%s' %i], self.drop[str(i)])\n",
        "                self.net1['A%s' %i] /= keep_prob[i]\n",
        "# bprob\n",
        "def bprop(self, batch_target,epoch,traing = False,keep_prob = None,regu = False,):\n",
        "        beta = 0.9\n",
        "        output_error = self.net1['A%s' %self.num_layers] - batch_target\n",
        "        self.grads['dW%s' %self.num_layers] = output_error.dot(self.net1['A%s' %(self.num_layers-1)].T)\n",
        "        self.grads['db%s' %self.num_layers] = np.sum(output_error, axis = 1, keepdims = True)\n",
        "        #l2 regulization\n",
        "        if(regu):\n",
        "                self.grads['dW%s'%self.num_layers] += self.lembda * self.parameters[\"W%s\" %self.num_layers] \n",
        "        #for momentum\n",
        "        self.lastgrads[\"dW%s\"%self.num_layers] = (beta * self.lastgrads[\"dW%s\"%self.num_layers]) + self.grads['dW%s' %self.num_layers]\n",
        "        self.lastgrads[\"db%s\"%self.num_layers] = (beta * self.lastgrads[\"db%s\"%self.num_layers]) + self.grads['db%s' %self.num_layers]\n",
        "        last_hidden_error = self.parameters['W%s'%self.num_layers] .T.dot(output_error)\n",
        "        for i in reversed(range(1, self.num_layers)):\n",
        "            dz = np.multiply(last_hidden_error,eval(self.activations_func[1]+'_derivative')(self.net['Z%s'%i]))\n",
        "            #Dropout\n",
        "            if(traing):\n",
        "                dz = np.multiply(dz, self.drop[str(i)])\n",
        "                dz /= keep_prob[i]\n",
        "            self.grads['dW%s' %i] = dz.dot(self.net1['A%s' %(i-1)].T)\n",
        "            self.grads['db%s' %i] = np.sum(dz, axis = 1, keepdims = True)\n",
        "            #l2 regulization\n",
        "            if(regu):\n",
        "                self.grads['dW%s' %i] += self.lembda * self.parameters[\"W%s\" %i] \n",
        "            #for momentum\n",
        "            self.lastgrads[\"dW%s\"%i] = (beta * self.lastgrads[\"dW%s\"%i]) + self.grads['dW%s' %i]\n",
        "            self.lastgrads[\"db%s\"%i] = (beta * self.lastgrads[\"db%s\"%i]) + self.grads['db%s' %i]\n",
        "            last_hidden_error = self.parameters['W%s'%i] .T.dot(dz)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}